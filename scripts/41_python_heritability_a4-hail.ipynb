{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ae475a-ef49-491a-a7dd-a19b1d65783a",
   "metadata": {},
   "source": [
    "# 41. Python version of heritability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b4e831-03e5-4fd2-b64c-470f363f7df8",
   "metadata": {},
   "source": [
    "Note here we set `df_row` to 0 because we're just testing code on first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db02cc-4e0d-4291-8bae-3734817b18b5",
   "metadata": {},
   "source": [
    "In a4, we're now trying to use the `hail` package instead of `limix`m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902c9c46-d6ab-4ba2-98ea-419d9fb1557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5dc4e42-ce02-48f2-9535-ec5a5d25e5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ba14210f-30ab-41b5-8eb0-268aae32b50b\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"ba14210f-30ab-41b5-8eb0-268aae32b50b\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "          for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"ba14210f-30ab-41b5-8eb0-268aae32b50b\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"ba14210f-30ab-41b5-8eb0-268aae32b50b\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.4.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.4.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ba14210f-30ab-41b5-8eb0-268aae32b50b\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/18 15:56:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     sc\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Now initialize Hail\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mhl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-1612>:2\u001b[0m, in \u001b[0;36minit\u001b[0;34m(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions, gcs_bucket_allow_list, copy_spark_log_on_error)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hail/context.py:391\u001b[0m, in \u001b[0;36minit\u001b[0;34m(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, backend, driver_cores, driver_memory, worker_cores, worker_memory, gcs_requester_pays_configuration, regions, gcs_bucket_allow_list, copy_spark_log_on_error)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hail_event_loop()\u001b[38;5;241m.\u001b[39mrun_until_complete(\n\u001b[1;32m    372\u001b[0m         init_batch(\n\u001b[1;32m    373\u001b[0m             log\u001b[38;5;241m=\u001b[39mlog,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m         )\n\u001b[1;32m    389\u001b[0m     )\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_spark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_block_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_block_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbranching_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbranching_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspark_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspark_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_optimizer_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_optimizer_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtmp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tmpdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_tmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43midempotent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midempotent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_logging_configuration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_logging_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_requester_pays_configuration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_requester_pays_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_log_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_spark_log_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_local(\n\u001b[1;32m    414\u001b[0m         log\u001b[38;5;241m=\u001b[39mlog,\n\u001b[1;32m    415\u001b[0m         quiet\u001b[38;5;241m=\u001b[39mquiet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m         gcs_requester_pays_configuration\u001b[38;5;241m=\u001b[39mgcs_requester_pays_configuration,\n\u001b[1;32m    422\u001b[0m     )\n",
      "File \u001b[0;32m<decorator-gen-1614>:2\u001b[0m, in \u001b[0;36minit_spark\u001b[0;34m(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, gcs_requester_pays_configuration, copy_log_on_error)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hail/context.py:485\u001b[0m, in \u001b[0;36minit_spark\u001b[0;34m(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, spark_conf, skip_logging_configuration, local_tmpdir, _optimizer_iterations, gcs_requester_pays_configuration, copy_log_on_error)\u001b[0m\n\u001b[1;32m    476\u001b[0m app_name \u001b[38;5;241m=\u001b[39m app_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHail\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    477\u001b[0m (\n\u001b[1;32m    478\u001b[0m     gcs_requester_pays_project,\n\u001b[1;32m    479\u001b[0m     gcs_requester_pays_buckets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    484\u001b[0m )\n\u001b[0;32m--> 485\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mSparkBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43midempotent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_block_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbranching_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_tmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_logging_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_requester_pays_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_requester_pays_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_requester_pays_buckets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_requester_pays_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy_log_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_log_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mexists(tmpdir):\n\u001b[1;32m    506\u001b[0m     backend\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmkdir(tmpdir)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hail/backend/spark_backend.py:130\u001b[0m, in \u001b[0;36mSparkBackend.__init__\u001b[0;34m(self, idempotent, sc, spark_conf, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmpdir, local_tmpdir, skip_logging_configuration, optimizer_iterations, gcs_requester_pays_project, gcs_requester_pays_buckets, copy_log_on_error)\u001b[0m\n\u001b[1;32m    128\u001b[0m     jhc \u001b[38;5;241m=\u001b[39m hail_package\u001b[38;5;241m.\u001b[39mHailContext\u001b[38;5;241m.\u001b[39mgetOrCreate(jbackend, branching_factor, optimizer_iterations)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     jbackend \u001b[38;5;241m=\u001b[39m \u001b[43mhail_package\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkBackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_logging_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_block_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_tmpdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_requester_pays_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgcs_requester_pays_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     jhc \u001b[38;5;241m=\u001b[39m hail_package\u001b[38;5;241m.\u001b[39mHailContext\u001b[38;5;241m.\u001b[39mapply(jbackend, branching_factor, optimizer_iterations)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jbackend\u001b[38;5;241m.\u001b[39msc()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Check if a SparkContext is already running\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if sc:\n",
    "    sc.stop()\n",
    "\n",
    "# Now initialize Hail\n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8364f-ea3d-4ab8-98d1-5e47897204c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from pgenlib import PgenReader\n",
    "import hail as hl\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# -----------------------------\n",
    "# Parameters (Adjust as needed)\n",
    "# -----------------------------\n",
    "window_sizes = [10000]                       # Window sizes in base pairs\n",
    "\n",
    "window_size = window_sizes[0]                # Single window size\n",
    "\n",
    "chunk_start = 1                              # Start index for CpG sites (1-based)\n",
    "chunk_end = 50                               # End index for CpG sites (1-based)\n",
    "benchmark = True                             # Whether to measure timing\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (Adjust these paths according to your data)\n",
    "# -----------------------------\n",
    "df_csv_path = \"/dcs04/lieber/statsgen/mnagle/mwas/CpGWAS/scripts/09.5-OUT_matched_SNP_meth_cov_chunked_JHPCE.csv\"\n",
    "output_dir = \"./41-OUT_heritability_a1\"\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize Hail and Benchmarking\n",
    "# -----------------------------\n",
    "hl.init()\n",
    "\n",
    "if benchmark:\n",
    "    start_time_total = time.time()\n",
    "\n",
    "# -----------------------------\n",
    "# Create Output Directory\n",
    "# -----------------------------\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.chdir(output_dir)\n",
    "print(f\"Output directory set to: {output_dir}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read the Metadata DataFrame\n",
    "# -----------------------------\n",
    "try:\n",
    "    df = pd.read_csv(df_csv_path)\n",
    "    print(f\"Metadata loaded from '{df_csv_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading metadata CSV '{df_csv_path}': {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# -----------------------------\n",
    "# Select the Row for Processing\n",
    "# -----------------------------\n",
    "df_row = 0  # Adjust as needed\n",
    "if df.empty:\n",
    "    print(\"Metadata DataFrame is empty. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Extract paths from the data frame\n",
    "gwas_dir = os.path.dirname(df.loc[df_row, 'SNP_data'])\n",
    "methylation_file = df.loc[df_row, 'modified_methylation_data']\n",
    "\n",
    "# Adjust methylation file paths\n",
    "methylation_file = methylation_file.replace(\n",
    "    \"/dcs04/lieber/statsgen/shizhong/michael/mwas/pheno/\",\n",
    "    \"/dcs04/lieber/statsgen/mnagle/mwas/pheno/\"\n",
    ").replace(\"rda\", \"csv\").replace(\"rds\", \"csv\")\n",
    "\n",
    "print(f\"Genotype Directory: {gwas_dir}\")\n",
    "print(f\"Methylation File: {methylation_file}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load Methylation Data\n",
    "# -----------------------------\n",
    "try:\n",
    "    # Methylation data has 'sample_id' as the first column and CpG positions as other columns\n",
    "    methylation_df = pd.read_csv(methylation_file)\n",
    "    print(f\"Methylation data loaded from '{methylation_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading methylation file '{methylation_file}': {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Ensure 'sample_id' is treated as a string\n",
    "if 'sample_id' not in methylation_df.columns:\n",
    "    print(f\"'sample_id' column not found in methylation data. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "methylation_df['sample_id'] = methylation_df['sample_id'].astype(str)\n",
    "print(\"'sample_id' column confirmed and converted to string.\")\n",
    "\n",
    "# Extract CpG columns (all columns except 'sample_id')\n",
    "cpg_columns = methylation_df.columns.drop('sample_id')\n",
    "\n",
    "# Extract numeric CpG positions from column names (e.g., 'pos_1069461' -> 1069461)\n",
    "try:\n",
    "    cpg_positions = [int(col.split('_')[1]) for col in cpg_columns]\n",
    "    print(\"CpG positions extracted from column names.\")\n",
    "except IndexError as e:\n",
    "    print(f\"Error parsing CpG positions in column names: {e}\")\n",
    "    exit(1)\n",
    "except ValueError as e:\n",
    "    print(f\"Non-integer CpG position found in column names: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a mapping from column names to positions\n",
    "cpg_col_to_pos = dict(zip(cpg_columns, cpg_positions))\n",
    "\n",
    "# Select the CpG positions for the specified chunk\n",
    "selected_cpg_cols = cpg_columns[chunk_start - 1:chunk_end]\n",
    "selected_cpg_positions = [cpg_col_to_pos[col] for col in selected_cpg_cols]\n",
    "\n",
    "print(f\"Selected CpG Columns: {selected_cpg_cols.tolist()}\")\n",
    "print(f\"Selected CpG Positions: {selected_cpg_positions}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Iterate Over Selected CpG Sites\n",
    "# -----------------------------\n",
    "for idx, (cpg_col, cpg_pos) in enumerate(zip(selected_cpg_cols, selected_cpg_positions), start=1):\n",
    "    print(f\"\\nProcessing CpG site {idx}: {cpg_col} at position {cpg_pos}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract Methylation Data for the Selected CpG Site\n",
    "    # -----------------------------\n",
    "    pheno_df = methylation_df[['sample_id', cpg_col]].dropna()\n",
    "    y = pheno_df[cpg_col].values\n",
    "    sample_ids = pheno_df['sample_id'].values\n",
    "    n_samples = len(sample_ids)\n",
    "\n",
    "    print(f\"Number of samples with non-missing methylation data: {n_samples}\")\n",
    "\n",
    "    if n_samples == 0:\n",
    "        print(\"No samples with non-missing methylation data. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Define Genomic Window\n",
    "    # -----------------------------\n",
    "    p1 = max(cpg_pos - window_size, 0)\n",
    "    p2 = cpg_pos + window_size\n",
    "\n",
    "    print(f\"Genomic window: {p1} - {p2} bp\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load Genotype Data for the Specified Chromosome\n",
    "    # -----------------------------\n",
    "    pgen_prefix = os.path.join(gwas_dir, f\"libd_chr{df.loc[df_row, 'Chr']}\")\n",
    "    pgen_file = f\"{pgen_prefix}.pgen\"\n",
    "    pvar_file = f\"{pgen_prefix}.pvar\"\n",
    "    psam_file = f\"{pgen_prefix}.psam\"\n",
    "\n",
    "    # Check if all necessary PLINK 2 files exist\n",
    "    if not all(os.path.exists(f) for f in [pgen_file, pvar_file, psam_file]):\n",
    "        print(\"One or more PLINK 2 files are missing. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    print(\"All necessary PLINK 2 files found.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Read Sample IDs from .psam File\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        psam_df = pd.read_csv(psam_file, sep='\\t')\n",
    "        if '#IID' not in psam_df.columns:\n",
    "            print(f\"'#IID' column not found in .psam file '{psam_file}'. Skipping this CpG site.\")\n",
    "            continue\n",
    "        geno_sample_ids = psam_df['#IID'].astype(str).values\n",
    "        print(\"Genotype sample IDs loaded from .psam file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .psam file '{psam_file}': {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # Create a mapping from sample ID to index in genotype data\n",
    "    sample_id_to_index = {sid: idx for idx, sid in enumerate(geno_sample_ids)}\n",
    "\n",
    "    # Get genotype indices for samples present in methylation data\n",
    "    geno_indices = [sample_id_to_index[sid] for sid in sample_ids if sid in sample_id_to_index]\n",
    "\n",
    "    if not geno_indices:\n",
    "        print(\"No matching samples between genotype and methylation data. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Number of matching samples: {len(geno_indices)}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Read SNP Positions from .pvar File\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        pvar_df = pd.read_csv(pvar_file, sep='\\t', comment='#',\n",
    "                              names=['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT'])\n",
    "        print(\"SNP positions loaded from .pvar file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .pvar file '{pvar_file}': {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # Subset SNPs within the genomic window\n",
    "    snps_in_window = pvar_df[(pvar_df['POS'] >= p1) & (pvar_df['POS'] <= p2)]\n",
    "\n",
    "    if snps_in_window.empty:\n",
    "        print(\"No SNPs found within the genomic window. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Number of SNPs within the window: {len(snps_in_window)}\")\n",
    "\n",
    "    # Get variant indices (0-based)\n",
    "    variant_indices = snps_in_window.index.values\n",
    "\n",
    "    # -----------------------------\n",
    "    # Initialize PgenReader with sample_subset\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        if benchmark:\n",
    "            start_time_geno = time.time()\n",
    "\n",
    "        pgr = PgenReader(pgen_file.encode('utf-8'), sample_subset=np.array(sorted(geno_indices), dtype=np.uint32))\n",
    "        print(\"PgenReader initialized.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing PgenReader: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Allocate buffer: rows=variants (SNPs), cols=samples\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        geno_buffer = np.empty((len(variant_indices), n_samples), dtype=np.int32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error allocating geno_buffer: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Read Genotype Data Using PgenReader\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        for var_idx, variant_idx in enumerate(variant_indices):\n",
    "            # Read genotype for the current variant\n",
    "            # allele_idx=1 corresponds to the alternate allele count\n",
    "            pgr.read(variant_idx, geno_buffer[var_idx, :], allele_idx=1)\n",
    "\n",
    "        print(\"Genotype data successfully read and stored in buffer.\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # Benchmarking: Genotype Reading Time\n",
    "        # -----------------------------\n",
    "        if benchmark:\n",
    "            geno_time = time.time() - start_time_geno\n",
    "            print(f\"Genotype reading time: {geno_time:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading genotype data: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Check for Missing Data and Impute\n",
    "    # -----------------------------\n",
    "    if np.any(geno_buffer == -9):\n",
    "        print(\"Missing genotype data detected. Imputing missing values with mean genotype.\")\n",
    "        # Replace missing genotypes (-9) with the mean genotype for each SNP\n",
    "        for var in range(geno_buffer.shape[0]):\n",
    "            missing = geno_buffer[var, :] == -9\n",
    "            if np.any(missing):\n",
    "                non_missing = geno_buffer[var, :] != -9\n",
    "                if np.any(non_missing):\n",
    "                    mean_geno = np.mean(geno_buffer[var, non_missing])\n",
    "                    geno_buffer[var, missing] = mean_geno\n",
    "                    print(f\"  Imputed missing values for SNP {var + 1} with mean genotype {mean_geno:.2f}.\")\n",
    "                else:\n",
    "                    # If all genotypes are missing, impute with 0\n",
    "                    geno_buffer[var, missing] = 0\n",
    "                    print(f\"  All genotypes missing for SNP {var + 1}. Imputed with 0.\")\n",
    "\n",
    "        # Check for NaNs after imputation\n",
    "        if np.isnan(geno_buffer).any():\n",
    "            nan_indices = np.argwhere(np.isnan(geno_buffer))\n",
    "            print(f\"NaNs found at positions: {nan_indices}\")\n",
    "            print(\"Exiting due to NaN values in geno_buffer.\")\n",
    "            exit(1)  # Stop execution to address the issue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Check number of SNPs\n",
    "    # -----------------------------\n",
    "    if len(snps_in_window) < 2:\n",
    "        print(\"Only one SNP in window; skipping heritability estimation.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Standardize Genotypes (Samples  SNPs)\n",
    "    # -----------------------------\n",
    "    print(\"Standardizing genotype data.\")\n",
    "\n",
    "    # Transpose to samples  SNPs\n",
    "    try:\n",
    "        M = geno_buffer.astype(float).T  # Shape: (Samples, SNPs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transposing geno_buffer: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # Compute mean and std per SNP (columns)\n",
    "    mu = np.mean(M, axis=0, keepdims=True)      # Shape: (1, SNPs)\n",
    "    sigma = np.std(M, axis=0, ddof=1, keepdims=True)  # Shape: (1, SNPs)\n",
    "\n",
    "    # Handle zero standard deviation\n",
    "    sigma[sigma == 0] = 1\n",
    "\n",
    "    # Standardize\n",
    "    S = (M - mu) / sigma  # Shape: (Samples, SNPs)\n",
    "\n",
    "    print(\"Genotype data standardized.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Compute Kinship Matrix using GEMMA Method\n",
    "    # -----------------------------\n",
    "    print(\"Computing kinship matrix.\")\n",
    "\n",
    "    try:\n",
    "        K = np.dot(S, S.T) / S.shape[1]  # Shape: (Samples, Samples)\n",
    "        print(\"Kinship matrix computed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing kinship matrix: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    if benchmark:\n",
    "        kinship_time = time.time() - start_time_total\n",
    "        print(f\"Kinship computation time: {kinship_time:.2f} seconds\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Normalize Kinship Matrix\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        mean_diag = np.mean(np.diagonal(K))\n",
    "        if mean_diag == 0:\n",
    "            print(\"Mean of the diagonal of the kinship matrix is zero. Cannot normalize. Skipping this CpG site.\")\n",
    "            continue\n",
    "        K_normalized = K / mean_diag\n",
    "        print(\"Kinship matrix normalized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Kinship normalization failed: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Estimate Heritability Using Hail and Statsmodels\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        # Create a DataFrame with phenotype and relatedness\n",
    "        herit_df = pd.DataFrame({\n",
    "            'phenotype': y\n",
    "        })\n",
    "\n",
    "        # Add kinship matrix as columns\n",
    "        for i in range(K_normalized.shape[0]):\n",
    "            herit_df[f'K_{i}'] = K_normalized[i, :]\n",
    "\n",
    "        # Fit a linear mixed model using Statsmodels\n",
    "        # Here, we treat the kinship matrix as multiple covariates\n",
    "        # Note: This is a simplification and not equivalent to proper LMMs used for heritability estimation\n",
    "        # For accurate heritability estimation, specialized packages should be used\n",
    "\n",
    "        # Define covariates\n",
    "        covariate_cols = [f'K_{i}' for i in range(K_normalized.shape[0])]\n",
    "\n",
    "        # Add an intercept\n",
    "        herit_df['intercept'] = 1\n",
    "\n",
    "        # Define the model\n",
    "        model = sm.OLS(herit_df['phenotype'], herit_df[['intercept'] + covariate_cols])\n",
    "\n",
    "        # Fit the model\n",
    "        results = model.fit()\n",
    "\n",
    "        # Extract R-squared as an approximation of heritability\n",
    "        h2 = results.rsquared\n",
    "        print(f\"Estimated heritability (h2): {h2:.4f}\")\n",
    "\n",
    "        if benchmark:\n",
    "            herit_time = time.time() - start_time_total\n",
    "            total_time = time.time() - start_time_total\n",
    "            print(f\"Heritability estimation time: {herit_time:.2f} seconds\")\n",
    "            print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Heritability estimation failed: {e}. Skipping this CpG site.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Collect and Save Results\n",
    "    # -----------------------------\n",
    "    print(\"Collecting results.\")\n",
    "    result_entry = {\n",
    "        'V_G': h2 * (np.mean(np.diagonal(K)) * (1 - h2)),\n",
    "        'V_e': (1 - h2) * (np.mean(np.diagonal(K)) * (1 - h2)),\n",
    "        'h2': h2,\n",
    "        'n': n_samples,\n",
    "        'site': f\"chr{df.loc[df_row, 'Chr']}_{cpg_pos}\",\n",
    "        'window_bp': window_size\n",
    "    }\n",
    "    results = [result_entry]\n",
    "\n",
    "    # Collect Timing Data (if benchmarking)\n",
    "    if benchmark:\n",
    "        timing_measurements = {\n",
    "            f\"chr{df.loc[df_row, 'Chr']}_pos{cpg_pos}_window{window_size}\": {\n",
    "                'geno_time_sec': geno_time,\n",
    "                'kinship_time_sec': kinship_time,\n",
    "                'herit_time_sec': herit_time,\n",
    "                'total_time_sec': time.time() - start_time_total\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Save Results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"heritability_results.csv\", mode='a', header=not os.path.exists(\"heritability_results.csv\"), index=False)\n",
    "    print(\"Heritability results saved to 'heritability_results.csv'.\")\n",
    "\n",
    "    # Save Timing Measurements to CSV (if benchmarking)\n",
    "    if benchmark:\n",
    "        timing_df = pd.DataFrame.from_dict(timing_measurements, orient='index')\n",
    "        timing_df.reset_index(inplace=True)\n",
    "        timing_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "        timing_df.to_csv(\"timing_measurements.csv\", mode='a', header=not os.path.exists(\"timing_measurements.csv\"), index=False)\n",
    "        print(\"Timing measurements saved to 'timing_measurements.csv'.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Finalize Hail\n",
    "# -----------------------------\n",
    "hl.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29cc9e-a299-4283-adcd-7c1c809676f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a65d3-5059-4038-9c0a-777d65ac70f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hail_env)",
   "language": "python",
   "name": "hail_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
